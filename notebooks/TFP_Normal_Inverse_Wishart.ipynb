{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFP Normal Inverse Wishart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPocKGVrZFdyVUENxvfGH58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lindermanlab/hackathons/blob/master/notebooks/TFP_Normal_Inverse_Wishart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKksHBVieJ-m"
      },
      "source": [
        "# Implementing a Normal Inverse Wishart Distribution in Tensorflow Probability\n",
        "_Scott Linderman_\n",
        "\n",
        "_Oct 27, 2021_\n",
        "\n",
        "---\n",
        "\n",
        "The normal inverse Wishart (NIW) is a conjugate prior for a multivariate Gaussian with unknown mean and covariance. Specifically, it's a joint distribution on a vector $\\mu \\in \\mathbb{R}^D$ and a positive semi-definite matrix $\\Sigma \\in \\mathbb{R}_{\\succeq 0}^{D \\times D}$ defined by the following generative model,\n",
        "\\begin{align}\n",
        "\\Sigma &\\sim \\mathrm{IW}(\\nu, \\Psi) \\\\\n",
        "\\mu \\mid \\Sigma &\\sim \\mathcal{N}(\\mu_0, \\kappa^{-1} \\Sigma)\n",
        "\\end{align}\n",
        "The NIW is parameterized by the degrees of freedom $\\nu$, a scale matrix $\\Psi$, a location $\\mu_0$, and a scaling parameter $\\kappa$ that I'll call the \"mean precision.\"\n",
        "Unfortunately, tensorflow probability (TFP) doesn't have a premade NIW distribution and implementing it was a bit of a pain... this notebook shows how I ended up doing it.\n",
        "\n",
        "TFP's `JointDistribution` objects seem well suited to this problem. Ideally, it would be as simple as,\n",
        "```\n",
        "niw = tfd.JointDistributionNamed(dict(\n",
        "    Sigma=lambda: tfd.InverseWishartTriL(df, np.linalg.cholesky(scale)), \n",
        "    mu=lambda Sigma: tfd.MultivariateNormalFullCovariance(loc, Sigma / mean_precision)\n",
        "))\n",
        "```\n",
        "\n",
        "The first problem is that TFP doesn't have an `InverseWishartTriL` distribution (ugh!). Ok, we'll just specify the inverse Wishart as a `TransformedDistribution` that wraps the `WishartTriL` distribution... something like,\n",
        "```\n",
        "niw = tfd.JointDistributionNamed(dict(\n",
        "    Sigma=lambda: tfd.TransformedDistribution(\n",
        "        tfd.WishartTriL(df, np.linalg.cholesky(np.linalg.inv(scale))),\n",
        "        bijector=tfb.MatrixInverse()), \n",
        "    mu=lambda Sigma: tfd.MultivariateNormalFullCovariance(loc, Sigma / mean_precision)\n",
        "))\n",
        "```\n",
        "\n",
        "The second problem is that TFP doesn't have a `MatrixInverse` bijector (ugh!!). Here's the workaround I arrived at: invert the (positive semidefinite) matrix $J$ output by the Wishart distribution by chaining three bijectors:\n",
        "1. $J \\mapsto \\mathrm{chol}(J)$                        \n",
        "2. $\\mathrm{chol}(J) \\mapsto \\mathrm{chol}(J^{-1})$ \n",
        "2. $\\mathrm{chol}(J^{-1}) \\mapsto \\mathrm{chol}(J^{-1}) \\mathrm{chol}(J^{-1})^\\top = J^{-1} \\equiv \\Sigma$ \n",
        "\n",
        "That would look like,\n",
        "```\n",
        "niw = tfd.JointDistributionNamed(dict(\n",
        "    Sigma=lambda: tfd.TransformedDistribution(\n",
        "        tfd.WishartTriL(df, np.linalg.cholesky(np.linalg.inv(scale))),\n",
        "        bijector=tfb.Chain([\n",
        "            tfb.CholeskyOuterProduct(),\n",
        "            tfb.CholeskyToInvCholesky(),\n",
        "            tfb.Cholesky()])), \n",
        "    mu=lambda Sigma: tfd.MultivariateNormalFullCovariance(loc, Sigma / mean_precision)\n",
        "))\n",
        "```\n",
        "\n",
        "The third problem is that TFP doesn't have a `Cholesky` bijector (ugh!!!). Thankfully, the Cholesky bijector is just the inverse of the `CholeskyOuterProduct` bijector, and we can use `tfb.Invert` to get it.\n",
        "\n",
        "The code below puts all of this together. I'm sure there are other implementations. For example, the TFP [Bayesian GMM example](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb) defines a new multivariate normal distribution parameterized by the square root of the inverse covariance (aka precision) matrix. That's probably a bit more efficient since it doesn't have so many conversions to/from Cholesky decompositions. Personally, I prefer the approach below because it outputs a distribution on $(\\mu, \\Sigma)$, as the NIW is typically specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOagFdceGT8"
      },
      "source": [
        "import jax.numpy as np\n",
        "import jax.random as jr\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfjxWK_cf7KC",
        "outputId": "7a6e6c0a-446f-407d-93f9-f3d3e2fc2025"
      },
      "source": [
        "dim = 3\n",
        "params = dict(\n",
        "    loc=np.zeros(dim),\n",
        "    mean_precision=1.0,\n",
        "    df=dim + 3,\n",
        "    scale=np.eye(dim)\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFhRtfa7qzTU",
        "outputId": "975c92d1-76a5-45a8-9de6-e6828e171722"
      },
      "source": [
        "def make_niw(loc, mean_precision, df, scale):\n",
        "    \"\"\"\n",
        "    loc: \\mu_0 in math above\n",
        "    mean_precision: \\kappa_0\n",
        "    df: \\nu\n",
        "    scale: \\Psi \n",
        "    \"\"\"\n",
        "    wishart_scale_tril = np.linalg.cholesky(np.linalg.inv(scale))\n",
        "    niw = tfd.JointDistributionNamed(dict(\n",
        "        Sigma=lambda: tfd.TransformedDistribution(\n",
        "            tfd.WishartTriL(df, scale_tril=wishart_scale_tril),\n",
        "            tfb.Chain([tfb.CholeskyOuterProduct(),                 \n",
        "                       tfb.CholeskyToInvCholesky(),                \n",
        "                       tfb.Invert(tfb.CholeskyOuterProduct())\n",
        "                       ])),\n",
        "        mu=lambda Sigma: tfd.MultivariateNormalFullCovariance(\n",
        "            loc, Sigma / mean_precision)\n",
        "    ))\n",
        "    return niw\n",
        "\n",
        "niw = make_niw(**params)\n",
        "smpl = niw.sample(seed=jr.PRNGKey(0))\n",
        "print(smpl)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sigma': DeviceArray([[ 0.39998385, -0.08702334, -0.01624717],\n",
            "             [-0.08702334,  0.28204948, -0.0854842 ],\n",
            "             [-0.01624717, -0.0854842 ,  0.18808396]], dtype=float32), 'mu': DeviceArray([0.47917017, 0.43772495, 0.16959237], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6UHLVM6v_zk"
      },
      "source": [
        "## Double check the log prob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NNO6DejiRH"
      },
      "source": [
        "from scipy.stats import invwishart\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "\n",
        "def manual_niw_log_prob(mu, Sigma, loc, mean_precision, df, scale):\n",
        "    lp = invwishart.logpdf(Sigma, df, scale)\n",
        "    lp += mvn.logpdf(mu, loc, Sigma / mean_precision)\n",
        "    return lp\n",
        "\n",
        "manual_niw_log_prob(**smpl, **params)\n",
        "assert np.allclose(niw.log_prob(smpl), manual_niw_log_prob(**smpl, **params))"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}